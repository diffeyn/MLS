{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99af75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas python-dateutil unidecode\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "from unidecode import unidecode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8790726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = r\"G://My Drive/GitHubProjects/MLS/data/data_clean/players\"  # e.g. r\"D:\\mls\\rosters\"\n",
    "OUTPUT_DIR = r\"G://My Drive/GitHubProjects/MLS/data/data_clean/players/sql_players\"         # e.g. r\"D:\\mls\\outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de2d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv(r\"G://My Drive/GitHubProjects/MLS/data/db_save/allteams.csv\")\n",
    "\n",
    "TEAM_ID = dict(zip(teams['team_name'], teams['team_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edce523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_team_name(name: str) -> str:\n",
    "    \"\"\"Lowercase, strip accents, trim space, unify dashes, squeeze spaces.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    s = unidecode(str(name)).strip().lower()\n",
    "    s = re.sub(r\"[-_]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"montreal\", \"cf montreal\")\n",
    "    s = s.replace(\"st. louis city sc\", \"st louis city sc\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d7a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_from_filename(fname: str) -> pd.Timestamp:\n",
    "    base = os.path.basename(fname)\n",
    "    stem = re.sub(r\"\\.csv(?:\\.csv)?$\", \"\", base) \n",
    "    stem = re.sub(r\"^cleaned[_-]\", \"\", stem, flags=re.I)\n",
    "    m = re.search(r\"([A-Za-z]{3,9}[-_ ]\\d{1,2}[-_, ]\\d{4}|\\d{4}[-_]\\d{1,2}[-_]\\d{1,2})\", stem)\n",
    "    if not m:\n",
    "        return pd.to_datetime(parser.parse(stem, fuzzy=True).date())\n",
    "    token = m.group(1).replace(\"_\", \" \").replace(\"-\", \" \").replace(\",\", \" \")\n",
    "    return pd.to_datetime(parser.parse(token).date())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45e647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_team_ids(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add team_id from TEAM_ID mapping.\"\"\"\n",
    "    if \"team\" not in df.columns:\n",
    "        raise ValueError(\"CSV must have a 'team' column.\")\n",
    "    if not TEAM_ID:\n",
    "        raise ValueError(\"Fill TEAM_ID mapping before running.\")\n",
    "    norm_map = {normalize_team_name(k): v for k, v in TEAM_ID.items()}\n",
    "    df[\"_team_norm\"] = df[\"team\"].map(normalize_team_name)\n",
    "    df[\"team_id\"] = df[\"_team_norm\"].map(norm_map)\n",
    "    missing = df[df[\"team_id\"].isna()][\"team\"].unique()\n",
    "    if len(missing):\n",
    "        raise ValueError(f\"Unmapped team names detected: {missing.tolist()}\")\n",
    "    return df.drop(columns=[\"_team_norm\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788319da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_snapshot(path: str) -> pd.DataFrame:\n",
    "    snap_date = parse_date_from_filename(path)\n",
    "    usecols = None  \n",
    "    df = pd.read_csv(path, low_memory=False, usecols=usecols)\n",
    "    for c in [\"contract_start\", \"contract_end\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "    need = {\"id\", \"team\"}\n",
    "    missing = need - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{os.path.basename(path)} is missing columns: {missing}\")\n",
    "\n",
    "    df = df.rename(columns={\"id\": \"player_id\"})\n",
    "    df[\"snap_date\"] = pd.to_datetime(snap_date)\n",
    "\n",
    "    keep_cols = [\"player_id\", \"team\", \"snap_date\"]\n",
    "    if \"contract_start\" in df.columns: keep_cols.append(\"contract_start\")\n",
    "    if \"contract_end\" in df.columns: keep_cols.append(\"contract_end\")\n",
    "    df = df[keep_cols].dropna(subset=[\"player_id\"])\n",
    "    df[\"player_id\"] = pd.to_numeric(df[\"player_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df.dropna(subset=[\"player_id\"]).astype({\"player_id\": \"int64\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "539437fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(data_dir: str) -> pd.DataFrame:\n",
    "    paths = sorted(glob.glob(os.path.join(data_dir, \"*.csv\")))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No CSVs found under {data_dir}\")\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            frames.append(read_one_snapshot(p))\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {os.path.basename(p)} -> {e}\")\n",
    "    snap = pd.concat(frames, ignore_index=True)\n",
    "    snap = ensure_team_ids(snap)\n",
    "\n",
    "    snap = (\n",
    "        snap.drop_duplicates(subset=[\"snap_date\", \"team_id\", \"player_id\"])\n",
    "            .sort_values([\"player_id\", \"snap_date\", \"team_id\"])\n",
    "            .reset_index(drop=True)\n",
    "    )\n",
    "    return snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4299f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_first_seen(snap: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (\n",
    "        snap.groupby(\"player_id\", as_index=False)[\"snap_date\"]\n",
    "            .min()\n",
    "            .rename(columns={\"snap_date\": \"first_seen\"})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d42b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stints(snap: pd.DataFrame) -> pd.DataFrame:\n",
    "    s = snap.sort_values([\"player_id\", \"snap_date\", \"team_id\"]).copy()\n",
    "    s[\"prev_team\"] = s.groupby(\"player_id\")[\"team_id\"].shift()\n",
    "    s[\"jump\"] = (s[\"team_id\"] != s[\"prev_team\"]).astype(int)\n",
    "    s.loc[s.groupby(\"player_id\").head(1).index, \"jump\"] = 1\n",
    "    s[\"stint_id\"] = s.groupby(\"player_id\")[\"jump\"].cumsum()\n",
    "\n",
    "    stints = (\n",
    "        s.groupby([\"player_id\", \"stint_id\", \"team_id\"])\n",
    "         .agg(stint_start=(\"snap_date\", \"min\"),\n",
    "              stint_end=(\"snap_date\", \"max\"),\n",
    "              days_observed=(\"snap_date\", lambda x: (x.max() - x.min()).days + 1),\n",
    "              obs_count=(\"snap_date\", \"count\"))\n",
    "         .reset_index()\n",
    "         .sort_values([\"player_id\", \"stint_start\"])\n",
    "    )\n",
    "    return stints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287dfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transfers(stints: pd.DataFrame) -> pd.DataFrame:\n",
    "    st = stints.sort_values([\"player_id\", \"stint_start\"]).copy()\n",
    "    st[\"next_team\"]  = st.groupby(\"player_id\")[\"team_id\"].shift(-1)\n",
    "    st[\"next_start\"] = st.groupby(\"player_id\")[\"stint_start\"].shift(-1)\n",
    "    transfers = st[(st[\"next_team\"].notna()) & (st[\"team_id\"] != st[\"next_team\"])].copy()\n",
    "    transfers = transfers.rename(columns={\n",
    "        \"team_id\": \"from_team_id\",\n",
    "        \"stint_start\": \"from_start\",\n",
    "        \"stint_end\": \"from_end\",\n",
    "        \"next_team\": \"to_team_id\",\n",
    "        \"next_start\": \"transfer_date\"\n",
    "    })\n",
    "    transfers = transfers[[\"player_id\", \"from_team_id\", \"to_team_id\", \"from_start\", \"from_end\", \"transfer_date\"]]\n",
    "    return transfers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_last_seen_and_inactive(snap: pd.DataFrame, gap_days: int = 60) -> pd.DataFrame:\n",
    "    last_seen = snap.groupby(\"player_id\", as_index=False)[\"snap_date\"].max().rename(columns={\"snap_date\": \"last_seen\"})\n",
    "    global_max = snap[\"snap_date\"].max()\n",
    "    last_seen[\"inactive_flag\"] = last_seen[\"last_seen\"] <= (global_max - pd.Timedelta(days=gap_days))\n",
    "    last_seen[\"as_of\"] = global_max\n",
    "    return last_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5977da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] snapshots: 492,066 rows\n",
      "[done] outputs written.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    snapshots = build_snapshots(DATA_DIR)\n",
    "\n",
    "    snapshots_out = snapshots[[\"snap_date\", \"team_id\", \"player_id\"] + [c for c in [\"contract_start\",\"contract_end\"] if c in snapshots.columns]]\n",
    "    snapshots_out.to_parquet(os.path.join(OUTPUT_DIR, \"roster_snapshots.parquet\"), index=False)\n",
    "    snapshots_out.to_csv(os.path.join(OUTPUT_DIR, \"roster_snapshots.csv\"), index=False)\n",
    "    print(f\"[ok] snapshots: {len(snapshots_out):,} rows\")\n",
    "\n",
    "    first_seen = compute_first_seen(snapshots)\n",
    "    first_seen.to_csv(os.path.join(OUTPUT_DIR, \"first_seen.csv\"), index=False)\n",
    "\n",
    "    stints = compute_stints(snapshots)\n",
    "    stints.to_csv(os.path.join(OUTPUT_DIR, \"stints.csv\"), index=False)\n",
    "\n",
    "    transfers = compute_transfers(stints)\n",
    "    transfers.to_csv(os.path.join(OUTPUT_DIR, \"transfers.csv\"), index=False)\n",
    "\n",
    "    last_seen = compute_last_seen_and_inactive(snapshots, gap_days=60)\n",
    "    last_seen.to_csv(os.path.join(OUTPUT_DIR, \"last_seen_60day_inactive.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e44b190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G://My Drive/GitHubProjects/MLS/data/data_clean/players/sql_players/stints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62fc21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "db_string = 'mysql+mysqldb://root:root@127.0.0.1:2022/MLS'\n",
    "\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2967040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5612"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(name='team_roster', con=engine, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
