name: MLS weekly scrape

on:
  schedule:
    # This runs at 8:00 AM UTC every Monday.
    - cron: '0 8 * * 1'
  
  # This allows you to run the workflow manually from the Actions tab.
  workflow_dispatch:

permissions:
  contents: write    

jobs:
  build:
    # Use the standard, stable version of Ubuntu.
    runs-on: ubuntu-latest

    steps:
      # Step 1: Check out your repository's code.
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Set up a STABLE version of Python.
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # Step 3: Set up the Chrome browser for Selenium to use.
      - name: Set up Chrome
        uses: browser-actions/setup-chrome@v1

      # Step 3b: Set up ChromeDriver for Selenium
      - name: Set up ChromeDriver
        uses: nanasess/setup-chromedriver@v2

      # Step 4: Install all the Python libraries listed in your requirements.txt file.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 5: Execute your script using the Python interpreter.
      - name: Execute Python script
        env:
          SECRET_API_KEY: ${{ secrets.SECRET_API_KEY }}
        run: python src/scrapemls.py 

      # Step 6: Commit the new data files back to your repository.
      - name: Commit and push if it changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Automated: Scraped new MLS data"
          file_pattern: 'data/github_actions/**/*.csv'